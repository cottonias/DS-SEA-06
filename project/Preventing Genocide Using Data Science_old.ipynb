{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREVENTING GENOCIDE\n",
    "#### USING DATA SCIENCE TO IDENTIFY EARLY INDICATORS OF FUTURE STATE LED MASS KILLINGS\n",
    "\n",
    "Mark Cotton / DS-SEA-06 / 2017-05-15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Background\n",
    "==========\n",
    "\n",
    "On April 6, 1994, Rwandan President Juvenal Habyarimana, a Hutu, was\n",
    "shot down above Kigali airport. In the following 100 days, between\n",
    "800,000 to 1,000,000 (perhaps even 2 M) Tutsi and moderate Hutus were\n",
    "slaughter were killed by fellow Rwandans, with violence being encouraged\n",
    "by the state. This averaged to 6 men, women and children were murdered\n",
    "every minute of every hour of every day for 3 months<sup>1</sup>.\n",
    "\n",
    "Since the Holocaust during World War II, there have been numerous mass\n",
    "killings classified as genocide, from atrocities in Cambodia to Rwanda\n",
    "to Bangladesh to name a few, resulting in the slaughter of millions of\n",
    "people. According to GenocideWatch.org, genocides and other mass murders\n",
    "killed more people in the twentieth century than all the wars combined.\n",
    "\n",
    "When the killings began in Rwanda, the international community responded\n",
    "inadequately and was not able to prevent the hundreds of thousands of\n",
    "deaths that followed. Did the world see this catastrophe coming? Could\n",
    "this all have been prevent or at least could the damage have been\n",
    "minimized? The biggest question is this: how do we prevent history from\n",
    "repeating itself again?\n",
    "\n",
    "What can be done to prevent genocide? From a report by the Genocide\n",
    "Prevention Task Force from the United States Institute of Peace,\n",
    "\n",
    "> *“The first major element of a comprehensive system to prevent\n",
    "> genocide and mass atrocities is a reliable process for accessing risks\n",
    "> and generating early warning of potential atrocities... Effective\n",
    "> early warning does not guarantee successful prevention, but if warning\n",
    "> is absent, slow, inaccurate, or indistinguishable from the 'noise' of\n",
    "> regular reporting, failure is virtually guaranteed.<sup>2</sup>”*\n",
    "\n",
    "What follows, per the Early Warning Project, is that genocide is nearly\n",
    "always carried out by a country’s own police and military forces,\n",
    "requiring external (international) intervention. To make predictions\n",
    "before a country has fallen in to a large-scale genocide, the marker to\n",
    "watch for is state-led mass killings. State-led mass killings occur when\n",
    "a country uses its own forces to kill over 1000 non-combatant civilians\n",
    "in a 12-month time period (in a country of over 500,000)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Problem statement and hypothesis\n",
    "================================\n",
    "\n",
    "Identifying early indicators that mass killings are going to occur will\n",
    "allow a better opportunity for prevention.\n",
    "\n",
    "Can data science machine learning principles be used to create a model\n",
    "to predict countries with a high likelihood of having a new state-led\n",
    "mass killing occur before the killings occur? The EWP already has\n",
    "created model to make these predictions. My hypothesis is that from\n",
    "within this data, certain combinations of features will be highly\n",
    "indicative of new mass killings occurring. I predict that lingual\n",
    "fractionalization, ethnic fractionalization, GDP, trade openness,\n",
    "country age, availability of natural resources, and history of mass\n",
    "killings will have strong correlations with new mass killings occurring.\n",
    "The model created will be in the form a logistic regression, predicting\n",
    "a 1 or 0 for new mass killings occurring in the following year, with\n",
    "some confidence percentage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Description of your data set and how it was obtained\n",
    "====================================================\n",
    "\n",
    "The data set used for this report is all directly pulled from a Github repo (available here: <https://github.com/EarlyWarningProject/2015-Statistical-Risk-Assessment>), where the Early Warning Project (EWP) has aggregated data from multiple sources. The EWP runs statistical risk assessment with the goal\n",
    "of preventing mass atrocities. The data from publicly available sources and was cleaned by EWP. This is a time series dataset spanning 178 countries from 1945-2014. This report is using the pre-processed dataset which includes 9330 observations (rows) with 276 variables (columns). The file “EWP Data Dictionary 20140909.pdf” describes the sources and variables. The information below is copied from that document outlining the sources:\n",
    "\n",
    "-   mkl = Early Warning Project’s Episodes of State-Led Mass Killing\n",
    "    Dataset\n",
    "\n",
    "-   wdi = World Bank’s World Development Indicators (via R\n",
    "    package ’WDI’)\n",
    "\n",
    "-   mev = Center for Systemic Peace’s Major Episodes of Political\n",
    "    Violence (<http://www.systemicpeace.org/inscrdata.html>)\n",
    "\n",
    "-   pol = Polity IV (<http://www.systemicpeace.org/inscrdata.html>)\n",
    "\n",
    "-   imr = U.S. Bureau of the Census, International Division (via PITF)\n",
    "\n",
    "-   cmm = Center for Systemic Peace’s List of Coups d’Etat\n",
    "    (<http://www.systemicpeace.org/inscrdata.html>)\n",
    "\n",
    "-   cpt = Powell and Thyne’s Coups d’Etat, 1950 to Present\n",
    "    (<http://www.uky.edu/~clthyn2/coup_data/home.htm>)\n",
    "\n",
    "-   cou = An amalgamation of cmm and cpt data\n",
    "\n",
    "-   pit = PITF Problem Set (i.e., episodes of instability)\n",
    "    (<http://www.systemicpeace.org/inscrdata.html>)\n",
    "\n",
    "-   dis = Center for Systemic Peace’s Discrimination Data Set (via PITF)\n",
    "\n",
    "-   imf = International Monetary Fund’s World Economic Outlook Report\n",
    "    (<http://www.imf.org/external/pubs/ft/weo/2014/01/weodata/index.aspx>)\n",
    "\n",
    "-   elf = Anderson’s Ethnic Fractionalization Data\n",
    "    (<http://www.anderson.ucla.edu/faculty_pages/romain.wacziarg/papersum.html>)\n",
    "\n",
    "-   hum = Farris et al.’s Latent Human Rights Protection Scores\n",
    "    (<http://humanrightsscores.org/>)\n",
    "\n",
    "-   fiw = Freedom House’s Freedom in the World Data Set\n",
    "    (<https://freedomhouse.org/report-types/freedom-world#.VA7Sufk7u-M>)\n",
    "\n",
    "-   aut = Geddes, Wright, and Frantz’s Authoritarian Regimes Dataset\n",
    "    (<http://sites.psu.edu/dictators/>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Import files, with the column headers cleaned so that '.' was replaced with '_'\n",
    "url = './data/ewp.statrisk.data.transformed.MC.csv'\n",
    "data = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>sftgcode</th>\n",
       "      <th>year</th>\n",
       "      <th>yrborn</th>\n",
       "      <th>yrdied</th>\n",
       "      <th>reg_eap</th>\n",
       "      <th>reg_afr</th>\n",
       "      <th>reg_eur</th>\n",
       "      <th>reg_mna</th>\n",
       "      <th>reg_sca</th>\n",
       "      <th>...</th>\n",
       "      <th>dis_l4pop_ln</th>\n",
       "      <th>imr_normed_ln</th>\n",
       "      <th>mev_regac</th>\n",
       "      <th>mev_regac_ln</th>\n",
       "      <th>mev_civtotc</th>\n",
       "      <th>mev_civtot_ln</th>\n",
       "      <th>imf_gdppcgrow</th>\n",
       "      <th>gdppcgrow</th>\n",
       "      <th>gdppcgrow_sr</th>\n",
       "      <th>slowgrowth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>1945</td>\n",
       "      <td>1919</td>\n",
       "      <td>9999</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>1946</td>\n",
       "      <td>1919</td>\n",
       "      <td>9999</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>1947</td>\n",
       "      <td>1919</td>\n",
       "      <td>9999</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.044522</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>1948</td>\n",
       "      <td>1919</td>\n",
       "      <td>9999</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.944439</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>1949</td>\n",
       "      <td>1919</td>\n",
       "      <td>9999</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.708050</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 243 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       country sftgcode  year  yrborn  yrdied  reg_eap  reg_afr  reg_eur  \\\n",
       "0  Afghanistan      AFG  1945    1919    9999        0        0        0   \n",
       "1  Afghanistan      AFG  1946    1919    9999        0        0        0   \n",
       "2  Afghanistan      AFG  1947    1919    9999        0        0        0   \n",
       "3  Afghanistan      AFG  1948    1919    9999        0        0        0   \n",
       "4  Afghanistan      AFG  1949    1919    9999        0        0        0   \n",
       "\n",
       "   reg_mna  reg_sca     ...      dis_l4pop_ln imr_normed_ln  mev_regac  \\\n",
       "0        0        1     ...               0.0           NaN        NaN   \n",
       "1        0        1     ...               0.0           NaN        8.0   \n",
       "2        0        1     ...               0.0           NaN       20.0   \n",
       "3        0        1     ...               0.0           NaN       18.0   \n",
       "4        0        1     ...               0.0           NaN       14.0   \n",
       "\n",
       "   mev_regac_ln  mev_civtotc  mev_civtot_ln  imf_gdppcgrow  gdppcgrow  \\\n",
       "0           NaN          NaN            NaN            NaN        NaN   \n",
       "1      2.197225          0.0            0.0            NaN        NaN   \n",
       "2      3.044522          0.0            0.0            NaN        NaN   \n",
       "3      2.944439          0.0            0.0            NaN        NaN   \n",
       "4      2.708050          0.0            0.0            NaN        NaN   \n",
       "\n",
       "   gdppcgrow_sr  slowgrowth  \n",
       "0           NaN         NaN  \n",
       "1           NaN         NaN  \n",
       "2           NaN         NaN  \n",
       "3           NaN         NaN  \n",
       "4           NaN         NaN  \n",
       "\n",
       "[5 rows x 243 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Description of any pre-processing steps you took\n",
    "================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9330 entries, 0 to 9329\n",
      "Columns: 243 entries, country to slowgrowth\n",
      "dtypes: float64(218), int64(21), object(4)\n",
      "memory usage: 17.3+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197200L"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the large number of features in these data, additional pre-processing is performed as part of step 6 for feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. What you learned from exploring the data, including visualizations\n",
    "==================================================================\n",
    "\n",
    "Some holes in data – information for history of country starts at 0\n",
    "generally (ie, mkl\\_ever) if the country is born in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many new state led mass killings occurred between 1945 and 2014?\n",
    "data[data.mkl_start == 1].mkl_start.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mkl_ongoing</th>\n",
       "      <th>mkl_start</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1945</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1946</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1947</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1948</th>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1949</th>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mkl_ongoing  mkl_start\n",
       "year                        \n",
       "1945            7          5\n",
       "1946            8          1\n",
       "1947            8          1\n",
       "1948           12          5\n",
       "1949           14          2"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Primary response is mkl-start, indicating a mass killing started that year and mkl-ongoing tracks ongoing mass killings\n",
    "df = data[data.mkl_ongoing == 1].groupby('year')['mkl_ongoing', 'mkl_start'].sum()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0xf6e95f8>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf4AAAF9CAYAAADlSwpTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6HTGQu2p/QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x159b9438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mkl_ongoing    34\n",
       "mkl_start       7\n",
       "dtype: int64"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year\n",
       "1991    7.0\n",
       "Name: mkl_start, dtype: float64"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Year with most mass killings started\n",
    "df[df == df.max()].mkl_start.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year\n",
       "1992    34.0\n",
       "Name: mkl_ongoing, dtype: float64"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Year with most mass killings ongoing\n",
    "df[df == df.max()].mkl_ongoing.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total countries with no state led mass killings since 1945: 106\n",
      "Total countries: 178\n"
     ]
    }
   ],
   "source": [
    "dft = data.groupby('country').sum()\n",
    "total_countries_no_mkl = dft[dft['mkl_ever'] == 0].mkl_ever.value_counts()[0]\n",
    "total_countries = dft.mkl_ever.count()\n",
    "print 'Total countries with no state led mass killings since 1945: %s' % total_countries_no_mkl\n",
    "print 'Total countries: %s' % total_countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    9214\n",
      "1     116\n",
      "Name: mkl_start, dtype: int64\n",
      "\n",
      "The null accuracy that any country in a given year will have a new MKL start is: 0.0124330117899\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# What is the null model, that is the number of new mkl/year?\n",
    "val = data.mkl_start.value_counts()\n",
    "print val\n",
    "null_accuracy = val[1]*1.0/val.sum()\n",
    "\n",
    "print\n",
    "print \"The null accuracy that any country in a given year will have a new MKL start is: %s\" % null_accuracy\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country\n",
       "Indonesia         5\n",
       "Iraq              4\n",
       "Sudan             4\n",
       "Congo-Kinshasa    4\n",
       "Nigeria           4\n",
       "Afghanistan       3\n",
       "Sri Lanka         3\n",
       "China             3\n",
       "Uganda            3\n",
       "Ethiopia          3\n",
       "Rwanda            3\n",
       "Philippines       3\n",
       "Syria             2\n",
       "Laos              2\n",
       "India             2\n",
       "Liberia           2\n",
       "Nicaragua         2\n",
       "Haiti             2\n",
       "Romania           2\n",
       "Iran              2\n",
       "Name: mkl_start, dtype: int64"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since data starting being collected, which varies from country to country and starts no sooner than 1945,\n",
    "# which countries have had the most mass killings start?\n",
    "# Note, this will not include instance where a country first appeared with ongoing mkl but no new mkl\n",
    "# (ie, in 1945, there were 7 ongoing mass killings and 5 started)\n",
    "# Also, countries which changed names, such as East Germany, West Germany and Germany, are counted individually here.\n",
    "data.groupby('country').mkl_start.sum().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What features are common between these countries?\n",
    "1. Birth year of country?\n",
    "1. GDP?\n",
    "1. Fractionalization?:\n",
    " *  Lingual (elf_language)\n",
    " *  Religious\n",
    " *  Ethnic\n",
    "1. Collapse of democratic institutions?\n",
    "1. Violence associated with regime transition?\n",
    "1. Democracy score? (pol_democ)\n",
    "1. Large supply of natural resources? Oil, gold, diamonds, etc.\n",
    " * Not sure oil is in the data or specific types of minerals.\n",
    " * Mineral depletion (wdi_minerals)\n",
    " * Forest depletion (wdi_forest)\n",
    " * Energy depletion (wdi_energy)\n",
    "1. Trade openness (wdi_trade)\n",
    "1. Context in which state-led mass killing episode began (mkl.type)\n",
    " * none\n",
    " * civil war\n",
    " * uprising\n",
    " * repression\n",
    " * other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country\n",
       "Uganda                      0.92\n",
       "Togo                        0.90\n",
       "Tanzania                    0.90\n",
       "Liberia                     0.90\n",
       "Kenya                       0.89\n",
       "Cameroon                    0.89\n",
       "Congo-Kinshasa              0.87\n",
       "South Africa                0.87\n",
       "Zambia                      0.87\n",
       "Chad                        0.86\n",
       "Nigeria                     0.85\n",
       "Mali                        0.84\n",
       "Philippines                 0.84\n",
       "Central African Republic    0.83\n",
       "Ethiopia                    0.81\n",
       "India                       0.81\n",
       "Mozambique                  0.81\n",
       "Gambia                      0.81\n",
       "Guinea-Bissau               0.81\n",
       "Benin                       0.79\n",
       "Name: elf_language, dtype: float64"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linguistic fractionalization index\n",
    "data.groupby('country').elf_language.max().sort_values(ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country\n",
      "Myanmar          13.875214\n",
      "United States    18.632225\n",
      "Brazil           18.649237\n",
      "Argentina        19.761625\n",
      "India            20.931200\n",
      "Japan            22.631970\n",
      "Bangladesh       27.135295\n",
      "Sudan            28.096628\n",
      "Ethiopia         30.323540\n",
      "Turkey           30.324656\n",
      "Name: wdi_trade, dtype: float64\n",
      "country\n",
      "Singapore               333.474066\n",
      "Equatorial Guinea       221.878705\n",
      "Bahrain                 162.914169\n",
      "Guyana                  161.468853\n",
      "Panama                  153.635176\n",
      "Swaziland               147.663333\n",
      "Lesotho                 142.375337\n",
      "Slovakia                136.596899\n",
      "United Arab Emirates    134.088522\n",
      "Malaysia                132.656492\n",
      "Name: wdi_trade, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Trade openness, lower is more open\n",
    "\n",
    "print data.groupby('country').wdi_trade.mean().sort_values(ascending = True).head(10)\n",
    "print data.groupby('country').wdi_trade.mean().sort_values(ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indonesia trade openness:\n",
      "Max: 96.18619424\n",
      "Min: 10.92290909\n",
      "Mean: 46.0338016411\n",
      "\n",
      "Average trade openness for all countries all years: 71.3862089985\n",
      "Max for all countries all years: 531.7374352\n",
      "Min for all countries all years: 0.308802944\n"
     ]
    }
   ],
   "source": [
    "# Trade openness, lower is more open\n",
    "dft = data[data.country == 'Indonesia']\n",
    "#print dft.head()\n",
    "print \"Indonesia trade openness:\"\n",
    "print \"Max: %s\" % dft['wdi_trade'].max()\n",
    "print \"Min: %s\" % dft['wdi_trade'].min()\n",
    "print \"Mean: %s\" %dft['wdi_trade'].mean()\n",
    "print\n",
    "print \"Average trade openness for all countries all years: %s\" % data.wdi_trade.mean()\n",
    "print \"Max for all countries all years: %s\" % data.wdi_trade.max()\n",
    "print \"Min for all countries all years: %s\" % data.wdi_trade.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pol_durable_ln   -0.114954\n",
      "pol_polity       -0.098278\n",
      "pol_democ        -0.096686\n",
      "pol_polcomp      -0.095495\n",
      "pol_exconst      -0.093020\n",
      "pol_xconst       -0.093020\n",
      "wdi_gdppcgrow    -0.092581\n",
      "pol_parcomp      -0.091409\n",
      "pol_exrec        -0.091135\n",
      "pol_xrcomp       -0.090424\n",
      "Name: mkl_start, dtype: float64\n",
      "pit_any_ongoing     0.183674\n",
      "pit_gen_deathmag    0.188718\n",
      "pit_rev_onset       0.258489\n",
      "mkl_ongoing         0.263942\n",
      "pit_eth_onset       0.278938\n",
      "pit_any_onset       0.328419\n",
      "pit_cwar_onset      0.349247\n",
      "pit_gen_onset       0.360816\n",
      "mkl_type            0.850716\n",
      "mkl_start           1.000000\n",
      "Name: mkl_start, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "dft = data.corr().mkl_start.sort_values()\n",
    "print dft.head(10)\n",
    "print dft.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. How you chose which features to use in your analysis\n",
    "==========================================================\n",
    "\n",
    "Features selected a priori. The features selected make sense and generally match up with the correlations from the results above. Would like to use additional features but want to refine model to be more than 13% accurate with recommended features first. (Don’t want to optimize a broken model – want to find out what is broken first and fix that.)\n",
    "\n",
    "The EWP uses three different models to predict new mass killings. For the “general” model, Random Forests is used with “no feature selection involved. These models were specified a priori to represent existing theories on the causes of mass killings, so this \\[R\\] script is only used to demonstrate that the resulting models and the average of forecasts from them do have real predictive power.” Below is the description of the specific features selected ahead a priori:\n",
    "\n",
    "-   mkl\\_start: Onset of any episodes of state-led mass killing\n",
    "\n",
    "-   reg\\_afr: US Dept State region: Sub-Saharan Africa\n",
    "\n",
    "-   reg\\_eap: US Dept State region: East Asia and Pacific\n",
    "\n",
    "-   reg\\_eur: US Dept State region: Europe and Eurasia\n",
    "\n",
    "-   reg\\_mna: US Dept State region: Middle East and North Africa\n",
    "\n",
    "-   reg\\_sca: US Dept State region: South and Central Asia\n",
    "\n",
    "-   reg\\_amr: US Dept State region: Americas\n",
    "\n",
    "-   mkl\\_ongoing; Any ongoing episodes of state-led mass killing\n",
    "\n",
    "-   mkl\\_ever: Any state-led mass killing since WWII (cumulative)\n",
    "\n",
    "-   countryage\\_ln: Country age, logged\n",
    "\n",
    "-   wdi\\_popsize\\_ln: Population size, logged\n",
    "\n",
    "-   imr\\_normed\\_ln: Infant mortality rate relative to annual global\n",
    "    median, logged\n",
    "\n",
    "-   gdppcgrow\\_sr: Annual % change in GDP per capita, meld of IMF and\n",
    "    WDI, square root\n",
    "\n",
    "-   wdi\\_trade\\_ln: Trade openness, logged\n",
    "\n",
    "-   ios\\_iccpr1: ICCPR 1st Optional Protocol signatory\n",
    "\n",
    "-   postcw: Post-Cold War period (year ≥ 1991)\n",
    "\n",
    "-   pol\\_cat\\_fl1: Autocracy (Fearon and Laitin)\n",
    "\n",
    "-   pol\\_cat\\_fl2: Anocracy (Fearon and Laitin)\n",
    "\n",
    "-   pol\\_cat\\_fl3: Democracy (Fearon and Laitin)\n",
    "\n",
    "-   pol\\_cat\\_fl7: Other (Fearon and Laitin)\n",
    "\n",
    "-   pol\\_durable\\_ln: Regime duration, logged (Polity)\n",
    "\n",
    "-   dis\\_l4pop\\_ln: Percent of population subjected to state-led\n",
    "    discrimination, logged\n",
    "\n",
    "-   elf\\_ethnicc1: Ethnic fractionalization: low\n",
    "\n",
    "-   elf\\_ethnicc2: Ethnic fractionalization: medium\n",
    "\n",
    "    Note: Final category not needed: elf\\_ethnicc3: ethnic\n",
    "    fractionalization: high\n",
    "\n",
    "-   elf\\_ethnicc9: Ethnic fractionalization: missing\n",
    "\n",
    "-   elc\\_eleth1: Salient elite ethnicity: majority rule\n",
    "\n",
    "-   elc\\_eleth2: Salient elite ethnicity: minority rule\n",
    "\n",
    "    Note: Final category not needed: elc.elethc : Salient elite\n",
    "    ethnicity (yes/no)\n",
    "\n",
    "-   elc\\_eliti: Ruling elites espouse an exclusionary ideology\n",
    "\n",
    "-   cou\\_tries5d: Any coup attempts in past 5 years ((t-4) to (t))\n",
    "\n",
    "-   pit\\_sftpuhvl2\\_10\\_ln: Sum of max annual magnitudes of PITF\n",
    "    (Political Instability Task Force) instability other than genocide\n",
    "    from past 10 yrs ((t-9) to (t)), logged\n",
    "\n",
    "-   mev\\_regac\\_ln: Scalar measure of armed conflict in geographic\n",
    "    region, logged\n",
    "\n",
    "-   mev\\_civtot\\_ln: Scale of violent civil conflict, logged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>year</th>\n",
       "      <th>mkl_start</th>\n",
       "      <th>mkl_start_1</th>\n",
       "      <th>reg_afr</th>\n",
       "      <th>reg_eap</th>\n",
       "      <th>reg_eur</th>\n",
       "      <th>reg_mna</th>\n",
       "      <th>reg_sca</th>\n",
       "      <th>reg_amr</th>\n",
       "      <th>...</th>\n",
       "      <th>elf_ethnicc1</th>\n",
       "      <th>elf_ethnicc2</th>\n",
       "      <th>elf_ethnicc9</th>\n",
       "      <th>elc_eleth1</th>\n",
       "      <th>elc_eleth2</th>\n",
       "      <th>elc_eliti</th>\n",
       "      <th>cou_tries5d</th>\n",
       "      <th>pit_sftpuhvl2_10_ln</th>\n",
       "      <th>mev_regac_ln</th>\n",
       "      <th>mev_civtot_ln</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>1945</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>1946</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>1947</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.044522</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>1948</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.944439</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>1949</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.708050</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>1950</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.218876</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>1951</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.833213</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>1952</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.564949</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>1953</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.639057</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>1954</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       country  year  mkl_start  mkl_start_1  reg_afr  reg_eap  reg_eur  \\\n",
       "0  Afghanistan  1945          0          0.0        0        0        0   \n",
       "1  Afghanistan  1946          0          0.0        0        0        0   \n",
       "2  Afghanistan  1947          0          0.0        0        0        0   \n",
       "3  Afghanistan  1948          0          0.0        0        0        0   \n",
       "4  Afghanistan  1949          0          0.0        0        0        0   \n",
       "5  Afghanistan  1950          0          0.0        0        0        0   \n",
       "6  Afghanistan  1951          0          0.0        0        0        0   \n",
       "7  Afghanistan  1952          0          0.0        0        0        0   \n",
       "8  Afghanistan  1953          0          0.0        0        0        0   \n",
       "9  Afghanistan  1954          0          0.0        0        0        0   \n",
       "\n",
       "   reg_mna  reg_sca  reg_amr      ...        elf_ethnicc1  elf_ethnicc2  \\\n",
       "0        0        1        0      ...                   0             0   \n",
       "1        0        1        0      ...                   0             0   \n",
       "2        0        1        0      ...                   0             0   \n",
       "3        0        1        0      ...                   0             0   \n",
       "4        0        1        0      ...                   0             0   \n",
       "5        0        1        0      ...                   0             0   \n",
       "6        0        1        0      ...                   0             0   \n",
       "7        0        1        0      ...                   0             0   \n",
       "8        0        1        0      ...                   0             0   \n",
       "9        0        1        0      ...                   0             0   \n",
       "\n",
       "   elf_ethnicc9  elc_eleth1  elc_eleth2  elc_eliti  cou_tries5d  \\\n",
       "0             0         NaN         NaN        NaN          NaN   \n",
       "1             0         NaN         NaN        NaN          NaN   \n",
       "2             0         NaN         NaN        NaN          NaN   \n",
       "3             0         NaN         NaN        NaN          NaN   \n",
       "4             0         NaN         NaN        NaN          NaN   \n",
       "5             0         NaN         NaN        NaN          0.0   \n",
       "6             0         NaN         NaN        NaN          0.0   \n",
       "7             0         NaN         NaN        NaN          0.0   \n",
       "8             0         NaN         NaN        NaN          0.0   \n",
       "9             0         NaN         NaN        NaN          0.0   \n",
       "\n",
       "   pit_sftpuhvl2_10_ln  mev_regac_ln  mev_civtot_ln  \n",
       "0                  NaN           NaN            NaN  \n",
       "1                  NaN      2.197225            0.0  \n",
       "2                  NaN      3.044522            0.0  \n",
       "3                  NaN      2.944439            0.0  \n",
       "4                  NaN      2.708050            0.0  \n",
       "5                  NaN      3.218876            0.0  \n",
       "6                  NaN      2.833213            0.0  \n",
       "7                  NaN      2.564949            0.0  \n",
       "8                  NaN      2.639057            0.0  \n",
       "9                  NaN      2.302585            0.0  \n",
       "\n",
       "[10 rows x 35 columns]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = data[['mkl_start','mkl_start_1','reg_afr','reg_eap','reg_eur','reg_mna','reg_sca','reg_amr','mkl_ongoing',\\\n",
    "             'mkl_ever','countryage_ln','wdi_popsize_ln','imr_normed_ln','gdppcgrow_sr','wdi_trade_ln',\\\n",
    "             'ios_iccpr1','postcw','pol_cat_fl1','pol_cat_fl2','pol_cat_fl3','pol_cat_fl7','pol_durable_ln',\\\n",
    "             'dis_l4pop_ln','elf_ethnicc1','elf_ethnicc2','elf_ethnicc9','elc_eleth1','elc_eleth2','elc_eliti',\\\n",
    "             'cou_tries5d','pit_sftpuhvl2_10_ln','mev_regac_ln','mev_civtot_ln']]\n",
    "\n",
    "info = data[['country','year']]\n",
    "# Don't need country and year for modeling, so save as separate dataframe\n",
    "train_info = result = pd.concat([info, train], axis=1)\n",
    "train_info.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the features listed above, there were 14563 nulls, with 2669 nulls\n",
    "in wdi\\_trade\\_ln, 2295 in gdppcgrow\\_sr, 1685 in wdi\\_popsize\\_ln."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total nulls in filtered data: 14563\n",
      "\n",
      "mkl_start                 0\n",
      "reg_afr                   0\n",
      "reg_eap                   0\n",
      "reg_eur                   0\n",
      "reg_mna                   0\n",
      "reg_sca                   0\n",
      "reg_amr                   0\n",
      "mkl_ongoing               0\n",
      "mkl_ever                  0\n",
      "countryage_ln             0\n",
      "wdi_popsize_ln         1685\n",
      "imr_normed_ln           966\n",
      "gdppcgrow_sr           2295\n",
      "wdi_trade_ln           2669\n",
      "ios_iccpr1              994\n",
      "postcw                    0\n",
      "pol_cat_fl1             144\n",
      "pol_cat_fl2             144\n",
      "pol_cat_fl3             144\n",
      "pol_cat_fl7             144\n",
      "pol_durable_ln          175\n",
      "dis_l4pop_ln              0\n",
      "elf_ethnicc1              0\n",
      "elf_ethnicc2              0\n",
      "elf_ethnicc9              0\n",
      "elc_eleth1              934\n",
      "elc_eleth2              934\n",
      "elc_eliti               934\n",
      "cou_tries5d             360\n",
      "pit_sftpuhvl2_10_ln    1507\n",
      "mev_regac_ln            267\n",
      "mev_civtot_ln           267\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print '\\nTotal nulls in filtered data: %s' % train.isnull().sum().sum()\n",
    "print\n",
    "print train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the features listed above, there were 14563 nulls, with 2669 nulls in wdi\\_trade\\_ln, 2295 in gdppcgrow\\_sr, 1685 in wdi\\_popsize\\_ln.\n",
    "\n",
    "To attempt to clean this data, nulls are replaced with the average values.\n",
    "\n",
    "**Note:**  The issue with this process of imputing missing data is that the mean value may be very misrepresentative for a given country. For example, Yugoslavia has no population size information for any years – is the global mean going to be a fare estimate? This country turned in to Bosnia and Herzegovina, Croatia, Federal Republic of Yugoslavia (later Serbia and Montenegro), Macedonia, and Slovenia. Yugoslavia had mass killings start in 1945 and 1991 and Bosnia and Herzegovina in 1992. Even though mass killings happened in Yugoslavia before the country split in 1991, none of these countries have “history” of mass killings (mkl\\_ever = 0). (Bosnia and Herzegovina had mass killings start in the first year and therefore have mkl\\_ever=1.) Most striking to me from this data is that Yugoslavia (mkl\\_ever = 1) split in to multiple countries including the Federal Republic of Yugoslavia (mkl\\_ever = 0) which later became Serbia and Montenegro (mkl\\_ever = 1) and then became the separate countries Serbia (mkl\\_ever = 0) and Montenegro (mkl\\_ever = 0). *Both the Federal Republic of Yugoslavia and (Serbia and Montenegro) never had\n",
    "mkl\\_start=0 and yet Serbia and Montenegro have history of mass killings.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Whatcha gunna do with all those nulls?\n",
    "# # Ideally, model values for each country.\n",
    "# # ie, use KNN or DBSCAN clustering to group countries for every year\n",
    "# # Start by using mean values...\n",
    "# train_cleaned = train.fillna(train.mean())\n",
    "# train_cleaned.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkl_start              0\n",
      "reg_afr                0\n",
      "reg_eap                0\n",
      "reg_eur                0\n",
      "reg_mna                0\n",
      "reg_sca                0\n",
      "reg_amr                0\n",
      "mkl_ongoing            0\n",
      "mkl_ever               0\n",
      "countryage_ln          0\n",
      "wdi_popsize_ln         0\n",
      "imr_normed_ln          0\n",
      "gdppcgrow_sr           0\n",
      "wdi_trade_ln           0\n",
      "ios_iccpr1             0\n",
      "postcw                 0\n",
      "pol_cat_fl1            0\n",
      "pol_cat_fl2            0\n",
      "pol_cat_fl3            0\n",
      "pol_cat_fl7            0\n",
      "pol_durable_ln         0\n",
      "dis_l4pop_ln           0\n",
      "elf_ethnicc1           0\n",
      "elf_ethnicc2           0\n",
      "elf_ethnicc9           0\n",
      "elc_eleth1             0\n",
      "elc_eleth2             0\n",
      "elc_eliti              0\n",
      "cou_tries5d            0\n",
      "pit_sftpuhvl2_10_ln    0\n",
      "mev_regac_ln           0\n",
      "mev_civtot_ln          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Whatcha gunna do with all those nulls?\n",
    "# Ideally, model values for each country.\n",
    "# ie, use KNN or DBSCAN clustering to group countries for every year\n",
    "# Drop null cells.\n",
    "train_cleaned = train.dropna()\n",
    "print train_cleaned.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_cleaned.mkl_start.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6196, 32)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mkl_start</th>\n",
       "      <th>reg_afr</th>\n",
       "      <th>reg_eap</th>\n",
       "      <th>reg_eur</th>\n",
       "      <th>reg_mna</th>\n",
       "      <th>reg_sca</th>\n",
       "      <th>reg_amr</th>\n",
       "      <th>mkl_ongoing</th>\n",
       "      <th>mkl_ever</th>\n",
       "      <th>countryage_ln</th>\n",
       "      <th>...</th>\n",
       "      <th>elf_ethnicc1</th>\n",
       "      <th>elf_ethnicc2</th>\n",
       "      <th>elf_ethnicc9</th>\n",
       "      <th>elc_eleth1</th>\n",
       "      <th>elc_eleth2</th>\n",
       "      <th>elc_eliti</th>\n",
       "      <th>cou_tries5d</th>\n",
       "      <th>pit_sftpuhvl2_10_ln</th>\n",
       "      <th>mev_regac_ln</th>\n",
       "      <th>mev_civtot_ln</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.442651</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.713572</td>\n",
       "      <td>2.995732</td>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.454347</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.713572</td>\n",
       "      <td>3.178054</td>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.465908</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.713572</td>\n",
       "      <td>3.218876</td>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.477337</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.688879</td>\n",
       "      <td>3.218876</td>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.488636</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.688879</td>\n",
       "      <td>3.135494</td>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.499810</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.688879</td>\n",
       "      <td>3.135494</td>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.510860</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.688879</td>\n",
       "      <td>3.178054</td>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.521789</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.688879</td>\n",
       "      <td>3.044522</td>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.532599</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.688879</td>\n",
       "      <td>2.944439</td>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.543295</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.688879</td>\n",
       "      <td>2.944439</td>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.553877</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.688879</td>\n",
       "      <td>2.944439</td>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.248495</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.262680</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.945910</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.276666</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.290459</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.304065</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.317488</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.330733</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.343805</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.356709</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.369448</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.382027</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.394449</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.772589</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.406719</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.772589</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.418841</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.044522</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.430817</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.944439</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.442651</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.454347</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>1.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.465908</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>1.945910</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.477337</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>2.708050</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9299</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>2.890372</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9300</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.397895</td>\n",
       "      <td>2.833213</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9301</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.945910</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.564949</td>\n",
       "      <td>2.833213</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9302</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.772589</td>\n",
       "      <td>2.833213</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9303</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.772589</td>\n",
       "      <td>2.833213</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9304</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.772589</td>\n",
       "      <td>2.833213</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9305</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.397895</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.772589</td>\n",
       "      <td>2.833213</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9306</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.639057</td>\n",
       "      <td>2.833213</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9307</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.564949</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>2.944439</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9308</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.639057</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>2.639057</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9309</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.708050</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>2.564949</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9310</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.772589</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>2.564949</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9311</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.833213</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>2.890372</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9312</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.890372</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.772589</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9313</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.944439</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.772589</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9314</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.995732</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.772589</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9315</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.044522</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.564949</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9316</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.091042</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.564949</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9317</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.135494</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.639057</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9318</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.178054</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9319</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.218876</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.945910</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9320</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.258097</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.945910</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9321</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.295837</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9322</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.332205</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9323</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.367296</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9324</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.401197</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9325</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.433987</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9326</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.465736</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9327</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.496508</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9328</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.526361</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.945910</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6196 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      mkl_start  reg_afr  reg_eap  reg_eur  reg_mna  reg_sca  reg_amr  \\\n",
       "58            0        0        0        0        0        1        0   \n",
       "59            0        0        0        0        0        1        0   \n",
       "60            0        0        0        0        0        1        0   \n",
       "61            0        0        0        0        0        1        0   \n",
       "62            0        0        0        0        0        1        0   \n",
       "63            0        0        0        0        0        1        0   \n",
       "64            0        0        0        0        0        1        0   \n",
       "65            0        0        0        0        0        1        0   \n",
       "66            0        0        0        0        0        1        0   \n",
       "67            0        0        0        0        0        1        0   \n",
       "68            0        0        0        0        0        1        0   \n",
       "106           0        0        0        1        0        0        0   \n",
       "107           0        0        0        1        0        0        0   \n",
       "108           0        0        0        1        0        0        0   \n",
       "109           0        0        0        1        0        0        0   \n",
       "110           0        0        0        1        0        0        0   \n",
       "111           0        0        0        1        0        0        0   \n",
       "112           0        0        0        1        0        0        0   \n",
       "113           0        0        0        1        0        0        0   \n",
       "114           0        0        0        1        0        0        0   \n",
       "115           0        0        0        1        0        0        0   \n",
       "116           0        0        0        1        0        0        0   \n",
       "117           0        0        0        1        0        0        0   \n",
       "118           0        0        0        1        0        0        0   \n",
       "119           0        0        0        1        0        0        0   \n",
       "120           0        0        0        1        0        0        0   \n",
       "121           0        0        0        1        0        0        0   \n",
       "122           0        0        0        1        0        0        0   \n",
       "123           0        0        0        1        0        0        0   \n",
       "124           0        0        0        1        0        0        0   \n",
       "...         ...      ...      ...      ...      ...      ...      ...   \n",
       "9299          0        1        0        0        0        0        0   \n",
       "9300          0        1        0        0        0        0        0   \n",
       "9301          0        1        0        0        0        0        0   \n",
       "9302          0        1        0        0        0        0        0   \n",
       "9303          0        1        0        0        0        0        0   \n",
       "9304          0        1        0        0        0        0        0   \n",
       "9305          0        1        0        0        0        0        0   \n",
       "9306          0        1        0        0        0        0        0   \n",
       "9307          0        1        0        0        0        0        0   \n",
       "9308          0        1        0        0        0        0        0   \n",
       "9309          0        1        0        0        0        0        0   \n",
       "9310          0        1        0        0        0        0        0   \n",
       "9311          0        1        0        0        0        0        0   \n",
       "9312          0        1        0        0        0        0        0   \n",
       "9313          0        1        0        0        0        0        0   \n",
       "9314          0        1        0        0        0        0        0   \n",
       "9315          0        1        0        0        0        0        0   \n",
       "9316          0        1        0        0        0        0        0   \n",
       "9317          0        1        0        0        0        0        0   \n",
       "9318          0        1        0        0        0        0        0   \n",
       "9319          0        1        0        0        0        0        0   \n",
       "9320          0        1        0        0        0        0        0   \n",
       "9321          0        1        0        0        0        0        0   \n",
       "9322          0        1        0        0        0        0        0   \n",
       "9323          0        1        0        0        0        0        0   \n",
       "9324          0        1        0        0        0        0        0   \n",
       "9325          0        1        0        0        0        0        0   \n",
       "9326          0        1        0        0        0        0        0   \n",
       "9327          0        1        0        0        0        0        0   \n",
       "9328          0        1        0        0        0        0        0   \n",
       "\n",
       "      mkl_ongoing  mkl_ever  countryage_ln      ...        elf_ethnicc1  \\\n",
       "58              0         1       4.442651      ...                   0   \n",
       "59              0         1       4.454347      ...                   0   \n",
       "60              0         1       4.465908      ...                   0   \n",
       "61              0         1       4.477337      ...                   0   \n",
       "62              0         1       4.488636      ...                   0   \n",
       "63              0         1       4.499810      ...                   0   \n",
       "64              0         1       4.510860      ...                   0   \n",
       "65              0         1       4.521789      ...                   0   \n",
       "66              0         1       4.532599      ...                   0   \n",
       "67              0         1       4.543295      ...                   0   \n",
       "68              0         1       4.553877      ...                   0   \n",
       "106             1         1       4.248495      ...                   1   \n",
       "107             1         1       4.262680      ...                   1   \n",
       "108             1         1       4.276666      ...                   1   \n",
       "109             1         1       4.290459      ...                   1   \n",
       "110             1         1       4.304065      ...                   1   \n",
       "111             0         1       4.317488      ...                   1   \n",
       "112             0         1       4.330733      ...                   1   \n",
       "113             0         1       4.343805      ...                   1   \n",
       "114             0         1       4.356709      ...                   1   \n",
       "115             0         1       4.369448      ...                   1   \n",
       "116             0         1       4.382027      ...                   1   \n",
       "117             0         1       4.394449      ...                   1   \n",
       "118             0         1       4.406719      ...                   1   \n",
       "119             0         1       4.418841      ...                   1   \n",
       "120             0         1       4.430817      ...                   1   \n",
       "121             0         1       4.442651      ...                   1   \n",
       "122             0         1       4.454347      ...                   1   \n",
       "123             0         1       4.465908      ...                   1   \n",
       "124             0         1       4.477337      ...                   1   \n",
       "...           ...       ...            ...      ...                 ...   \n",
       "9299            1         1       1.609438      ...                   0   \n",
       "9300            1         1       1.791759      ...                   0   \n",
       "9301            1         1       1.945910      ...                   0   \n",
       "9302            1         1       2.079442      ...                   0   \n",
       "9303            0         1       2.197225      ...                   0   \n",
       "9304            0         1       2.302585      ...                   0   \n",
       "9305            0         1       2.397895      ...                   0   \n",
       "9306            0         1       2.484907      ...                   0   \n",
       "9307            0         1       2.564949      ...                   0   \n",
       "9308            0         1       2.639057      ...                   0   \n",
       "9309            0         1       2.708050      ...                   0   \n",
       "9310            0         1       2.772589      ...                   0   \n",
       "9311            0         1       2.833213      ...                   0   \n",
       "9312            0         1       2.890372      ...                   0   \n",
       "9313            0         1       2.944439      ...                   0   \n",
       "9314            0         1       2.995732      ...                   0   \n",
       "9315            0         1       3.044522      ...                   0   \n",
       "9316            0         1       3.091042      ...                   0   \n",
       "9317            0         1       3.135494      ...                   0   \n",
       "9318            0         1       3.178054      ...                   0   \n",
       "9319            0         1       3.218876      ...                   0   \n",
       "9320            0         1       3.258097      ...                   0   \n",
       "9321            0         1       3.295837      ...                   0   \n",
       "9322            0         1       3.332205      ...                   0   \n",
       "9323            0         1       3.367296      ...                   0   \n",
       "9324            0         1       3.401197      ...                   0   \n",
       "9325            0         1       3.433987      ...                   0   \n",
       "9326            0         1       3.465736      ...                   0   \n",
       "9327            0         1       3.496508      ...                   0   \n",
       "9328            0         1       3.526361      ...                   0   \n",
       "\n",
       "      elf_ethnicc2  elf_ethnicc9  elc_eleth1  elc_eleth2  elc_eliti  \\\n",
       "58               0             0         0.0         0.0        0.0   \n",
       "59               0             0         0.0         0.0        0.0   \n",
       "60               0             0         0.0         0.0        0.0   \n",
       "61               0             0         0.0         0.0        0.0   \n",
       "62               0             0         0.0         0.0        0.0   \n",
       "63               0             0         0.0         0.0        0.0   \n",
       "64               0             0         0.0         0.0        0.0   \n",
       "65               0             0         0.0         0.0        0.0   \n",
       "66               0             0         0.0         0.0        0.0   \n",
       "67               0             0         0.0         0.0        0.0   \n",
       "68               0             0         0.0         0.0        0.0   \n",
       "106              0             0         0.0         0.0        1.0   \n",
       "107              0             0         0.0         0.0        1.0   \n",
       "108              0             0         0.0         0.0        1.0   \n",
       "109              0             0         0.0         0.0        1.0   \n",
       "110              0             0         0.0         0.0        1.0   \n",
       "111              0             0         0.0         0.0        1.0   \n",
       "112              0             0         0.0         0.0        1.0   \n",
       "113              0             0         0.0         0.0        1.0   \n",
       "114              0             0         0.0         0.0        1.0   \n",
       "115              0             0         0.0         0.0        1.0   \n",
       "116              0             0         0.0         0.0        0.0   \n",
       "117              0             0         0.0         0.0        0.0   \n",
       "118              0             0         0.0         0.0        0.0   \n",
       "119              0             0         0.0         0.0        0.0   \n",
       "120              0             0         0.0         0.0        0.0   \n",
       "121              0             0         0.0         0.0        0.0   \n",
       "122              0             0         0.0         0.0        0.0   \n",
       "123              0             0         0.0         0.0        0.0   \n",
       "124              0             0         0.0         0.0        0.0   \n",
       "...            ...           ...         ...         ...        ...   \n",
       "9299             1             0         1.0         0.0        0.0   \n",
       "9300             1             0         1.0         0.0        0.0   \n",
       "9301             1             0         1.0         0.0        0.0   \n",
       "9302             1             0         1.0         0.0        0.0   \n",
       "9303             1             0         1.0         0.0        0.0   \n",
       "9304             1             0         1.0         0.0        0.0   \n",
       "9305             1             0         1.0         0.0        0.0   \n",
       "9306             1             0         1.0         0.0        0.0   \n",
       "9307             1             0         1.0         0.0        0.0   \n",
       "9308             1             0         1.0         0.0        0.0   \n",
       "9309             1             0         1.0         0.0        0.0   \n",
       "9310             1             0         1.0         0.0        0.0   \n",
       "9311             1             0         1.0         0.0        0.0   \n",
       "9312             1             0         1.0         0.0        0.0   \n",
       "9313             1             0         1.0         0.0        0.0   \n",
       "9314             1             0         1.0         0.0        0.0   \n",
       "9315             1             0         1.0         0.0        0.0   \n",
       "9316             1             0         1.0         0.0        0.0   \n",
       "9317             1             0         1.0         0.0        0.0   \n",
       "9318             1             0         1.0         0.0        0.0   \n",
       "9319             1             0         1.0         0.0        0.0   \n",
       "9320             1             0         1.0         0.0        0.0   \n",
       "9321             1             0         1.0         0.0        0.0   \n",
       "9322             1             0         1.0         0.0        0.0   \n",
       "9323             1             0         1.0         0.0        0.0   \n",
       "9324             1             0         1.0         0.0        0.0   \n",
       "9325             1             0         1.0         0.0        0.0   \n",
       "9326             1             0         1.0         0.0        0.0   \n",
       "9327             1             0         1.0         0.0        0.0   \n",
       "9328             1             0         1.0         0.0        0.0   \n",
       "\n",
       "      cou_tries5d  pit_sftpuhvl2_10_ln  mev_regac_ln  mev_civtot_ln  \n",
       "58            1.0             3.713572      2.995732       1.386294  \n",
       "59            1.0             3.713572      3.178054       1.386294  \n",
       "60            1.0             3.713572      3.218876       1.386294  \n",
       "61            1.0             3.688879      3.218876       1.386294  \n",
       "62            0.0             3.688879      3.135494       1.386294  \n",
       "63            0.0             3.688879      3.135494       1.386294  \n",
       "64            0.0             3.688879      3.178054       1.386294  \n",
       "65            0.0             3.688879      3.044522       1.386294  \n",
       "66            0.0             3.688879      2.944439       1.386294  \n",
       "67            0.0             3.688879      2.944439       1.386294  \n",
       "68            0.0             3.688879      2.944439       1.386294  \n",
       "106           0.0             0.000000      1.791759       0.000000  \n",
       "107           0.0             0.000000      1.945910       0.000000  \n",
       "108           0.0             0.000000      1.609438       0.000000  \n",
       "109           0.0             0.000000      2.079442       0.000000  \n",
       "110           0.0             0.000000      2.079442       0.000000  \n",
       "111           0.0             0.000000      1.791759       0.000000  \n",
       "112           0.0             0.000000      1.791759       0.000000  \n",
       "113           0.0             0.000000      1.791759       0.000000  \n",
       "114           0.0             0.000000      2.079442       0.000000  \n",
       "115           0.0             0.000000      1.791759       0.000000  \n",
       "116           0.0             0.000000      2.197225       0.000000  \n",
       "117           0.0             0.000000      2.772589       0.000000  \n",
       "118           0.0             0.000000      2.772589       0.000000  \n",
       "119           0.0             0.000000      3.044522       0.000000  \n",
       "120           0.0             0.000000      2.944439       0.000000  \n",
       "121           0.0             1.386294      2.302585       0.000000  \n",
       "122           0.0             2.079442      1.791759       1.098612  \n",
       "123           1.0             2.079442      1.945910       0.000000  \n",
       "124           1.0             2.079442      2.708050       0.000000  \n",
       "...           ...                  ...           ...            ...  \n",
       "9299          0.0             2.197225      2.890372       0.693147  \n",
       "9300          0.0             2.397895      2.833213       0.693147  \n",
       "9301          0.0             2.564949      2.833213       0.693147  \n",
       "9302          0.0             2.772589      2.833213       0.693147  \n",
       "9303          0.0             2.772589      2.833213       0.000000  \n",
       "9304          0.0             2.772589      2.833213       0.000000  \n",
       "9305          0.0             2.772589      2.833213       0.000000  \n",
       "9306          0.0             2.639057      2.833213       0.000000  \n",
       "9307          0.0             2.484907      2.944439       0.000000  \n",
       "9308          0.0             2.302585      2.639057       0.000000  \n",
       "9309          0.0             2.079442      2.564949       0.000000  \n",
       "9310          0.0             1.791759      2.564949       0.000000  \n",
       "9311          0.0             1.386294      2.890372       0.000000  \n",
       "9312          0.0             0.000000      2.772589       0.000000  \n",
       "9313          0.0             0.000000      2.772589       0.000000  \n",
       "9314          0.0             0.000000      2.772589       0.000000  \n",
       "9315          0.0             0.000000      2.564949       0.000000  \n",
       "9316          0.0             0.000000      2.564949       0.000000  \n",
       "9317          0.0             0.000000      2.639057       0.000000  \n",
       "9318          0.0             0.000000      2.079442       0.000000  \n",
       "9319          0.0             0.000000      1.945910       0.000000  \n",
       "9320          0.0             0.000000      1.945910       0.000000  \n",
       "9321          0.0             0.000000      1.791759       0.000000  \n",
       "9322          0.0             0.000000      1.791759       0.000000  \n",
       "9323          0.0             0.000000      1.791759       0.000000  \n",
       "9324          0.0             0.000000      1.791759       0.000000  \n",
       "9325          0.0             0.000000      1.791759       0.000000  \n",
       "9326          0.0             0.000000      1.791759       0.000000  \n",
       "9327          0.0             0.000000      1.791759       0.000000  \n",
       "9328          0.0             0.000000      1.945910       0.000000  \n",
       "\n",
       "[6196 rows x 32 columns]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print train_cleaned.shape\n",
    "train_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following cells are attempting to create a new column for if a mass killing will start in the following year for that country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def buildLeadingFeatures(s,lead=2,dropna=True):\n",
    "    '''\n",
    "    Builds a new DataFrame to facilitate regressing over all possible lagged features\n",
    "    '''\n",
    "    if type(s) is pd.DataFrame:\n",
    "        new_dict={}\n",
    "        for col_name in s:\n",
    "            new_dict[col_name]=s[col_name]\n",
    "            # create leading Series\n",
    "            for l in range(1,lead+1):\n",
    "                new_dict['%s_lead%d' %(col_name,l)]=s[col_name].shift(-1)\n",
    "        res=pd.DataFrame(new_dict,index=s.index)\n",
    "\n",
    "    elif type(s) is pd.Series:\n",
    "        the_range=range(lead+1)\n",
    "        res=pd.concat([s.shift(-i) for i in the_range],axis=1)\n",
    "        res.columns=['lead_%d' %i for i in the_range]\n",
    "    else:\n",
    "        print 'Only works for DataFrame or Series'\n",
    "        return None\n",
    "    if dropna:\n",
    "        return res.dropna()\n",
    "    else:\n",
    "        return res\n",
    "\n",
    "#s2=pd.DataFrame({'a':[5,4,3,2,1], 'b':[50,40,30,20,10]},index=[1,2,3,4,5])\n",
    "#print s2\n",
    "#res2=buildLeadingFeatures(s2,lead=1,dropna=False)\n",
    "#res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>mkl_start</th>\n",
       "      <th>mkl_start_next_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Albania</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>Angola</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>Argentina</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>Armenia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>Australia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>Austria</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>Azerbaijan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>Bahamas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>Bahrain</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>Barbados</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>Belarus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>Belgium</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>Belize</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>Benin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>Bhutan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>Bolivia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>Bosnia and Herzegovina</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>Botswana</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>Bulgaria</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1205</th>\n",
       "      <td>Burkina Faso</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1260</th>\n",
       "      <td>Burundi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1313</th>\n",
       "      <td>Cambodia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1375</th>\n",
       "      <td>Cameroon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>Canada</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>Cape Verde</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1540</th>\n",
       "      <td>Central African Republic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7816</th>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7859</th>\n",
       "      <td>Sudan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7919</th>\n",
       "      <td>Swaziland</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7966</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8036</th>\n",
       "      <td>Switzerland</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8106</th>\n",
       "      <td>Syria</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8175</th>\n",
       "      <td>Taiwan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8241</th>\n",
       "      <td>Tajikistan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8265</th>\n",
       "      <td>Tanzania</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8319</th>\n",
       "      <td>Thailand</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8389</th>\n",
       "      <td>Timor Leste</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8402</th>\n",
       "      <td>Togo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8457</th>\n",
       "      <td>Trinidad and Tobago</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8510</th>\n",
       "      <td>Tunisia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8569</th>\n",
       "      <td>Turkey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8639</th>\n",
       "      <td>Turkmenistan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8663</th>\n",
       "      <td>Uganda</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8716</th>\n",
       "      <td>Ukraine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8740</th>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8784</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8854</th>\n",
       "      <td>United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8924</th>\n",
       "      <td>Uruguay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8994</th>\n",
       "      <td>Uzbekistan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9018</th>\n",
       "      <td>Venezuela</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9088</th>\n",
       "      <td>Vietnam</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9127</th>\n",
       "      <td>West Germany</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9172</th>\n",
       "      <td>Yemen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9197</th>\n",
       "      <td>Yugoslavia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9244</th>\n",
       "      <td>Zambia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9295</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       country  mkl_start  mkl_start_next_year\n",
       "0                  Afghanistan        NaN                  NaN\n",
       "70                     Albania        NaN                  NaN\n",
       "140                    Algeria        NaN                  0.0\n",
       "193                     Angola        NaN                  NaN\n",
       "233                  Argentina        NaN                  NaN\n",
       "303                    Armenia        NaN                  0.0\n",
       "327                  Australia        NaN                  NaN\n",
       "397                    Austria        NaN                  NaN\n",
       "467                 Azerbaijan        NaN                  0.0\n",
       "491                    Bahamas        NaN                  NaN\n",
       "533                    Bahrain        NaN                  NaN\n",
       "577                 Bangladesh        NaN                  0.0\n",
       "621                   Barbados        NaN                  NaN\n",
       "670                    Belarus        NaN                  0.0\n",
       "694                    Belgium        NaN                  NaN\n",
       "764                     Belize        NaN                  NaN\n",
       "798                      Benin        NaN                  0.0\n",
       "853                     Bhutan        NaN                  NaN\n",
       "923                    Bolivia        NaN                  NaN\n",
       "993     Bosnia and Herzegovina        NaN                  NaN\n",
       "1016                  Botswana        NaN                  0.0\n",
       "1065                    Brazil        NaN                  NaN\n",
       "1135                  Bulgaria        NaN                  NaN\n",
       "1205              Burkina Faso        NaN                  0.0\n",
       "1260                   Burundi        NaN                  0.0\n",
       "1313                  Cambodia        NaN                  NaN\n",
       "1375                  Cameroon        NaN                  NaN\n",
       "1430                    Canada        NaN                  NaN\n",
       "1500                Cape Verde        NaN                  NaN\n",
       "1540  Central African Republic        NaN                  0.0\n",
       "...                        ...        ...                  ...\n",
       "7816                 Sri Lanka        0.0                  0.0\n",
       "7859                     Sudan        NaN                  NaN\n",
       "7919                 Swaziland        NaN                  NaN\n",
       "7966                    Sweden        NaN                  NaN\n",
       "8036               Switzerland        NaN                  NaN\n",
       "8106                     Syria        NaN                  NaN\n",
       "8175                    Taiwan        NaN                  NaN\n",
       "8241                Tajikistan        NaN                  1.0\n",
       "8265                  Tanzania        NaN                  NaN\n",
       "8319                  Thailand        NaN                  NaN\n",
       "8389               Timor Leste        0.0                  0.0\n",
       "8402                      Togo        NaN                  0.0\n",
       "8457       Trinidad and Tobago        NaN                  0.0\n",
       "8510                   Tunisia        NaN                  NaN\n",
       "8569                    Turkey        NaN                  NaN\n",
       "8639              Turkmenistan        NaN                  0.0\n",
       "8663                    Uganda        NaN                  NaN\n",
       "8716                   Ukraine        NaN                  0.0\n",
       "8740      United Arab Emirates        NaN                  NaN\n",
       "8784            United Kingdom        NaN                  NaN\n",
       "8854             United States        NaN                  NaN\n",
       "8924                   Uruguay        NaN                  NaN\n",
       "8994                Uzbekistan        NaN                  0.0\n",
       "9018                 Venezuela        NaN                  NaN\n",
       "9088                   Vietnam        NaN                  NaN\n",
       "9127              West Germany        NaN                  NaN\n",
       "9172                     Yemen        NaN                  0.0\n",
       "9197                Yugoslavia        NaN                  NaN\n",
       "9244                    Zambia        NaN                  0.0\n",
       "9295                  Zimbabwe        0.0                  0.0\n",
       "\n",
       "[178 rows x 3 columns]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft = pd.concat([info,train_cleaned],axis=1)\n",
    "columns=['mkl_start','mkl_start_next_year']\n",
    "dft2 = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Create new dataframe sorted by country and create new column for mkl_start_next_year\n",
    "for nation in dft.groupby('country').country.head(1):\n",
    "    # Create response for mkl_start_next_year for each country\n",
    "    # This also drops last observation for country, as mkl_start_next_year is null\n",
    "    temp = buildLeadingFeatures(dft[dft.country == nation].mkl_start,lead=1,dropna=False)\n",
    "    temp.rename(columns={\"lead_0\": \"mkl_start\", \"lead_1\": \"mkl_start_next_year\"}, inplace = True)\n",
    "    # Append dataframe for country into running \n",
    "    dft2 = dft2.append(temp,ignore_index=False)\n",
    "\n",
    "dft3 = pd.concat([dft,dft2.mkl_start_next_year],axis=1)\n",
    "# Verify results by making sure a country with mkl_start in its first year \n",
    "# does not result in mkl_start_next_year = 1 for the previous row.\n",
    "#dft3[['country','mkl_start','mkl_start_next_year']][190:201]\n",
    "#for nation in dft.groupby('country').country.head(1):\n",
    "dft3[['country','mkl_start','mkl_start_next_year']].groupby('country').head(1)\n",
    "\n",
    "#dft3[190:201]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_cleaned = dft3\n",
    "print train_cleaned.shape\n",
    "print\n",
    "print \"Nulls counted in the new column: %s\" % train_cleaned.isnull().mkl_start_next_year.sum()\n",
    "\n",
    "print train_cleaned[train_cleaned.mkl_start_next_year.isnull() == True].head()\n",
    "# Drop rows where we do not have data on if a mkl started the next year, ie last year reported for country\n",
    "train_cleaned = train_cleaned.dropna()\n",
    "print \"Nulls after cleaning new column: %s\" % train_cleaned.isnull().mkl_start_next_year.sum()\n",
    "\n",
    "print train_cleaned.shape\n",
    "train_cleaned[188:198]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_count = train_cleaned[train_cleaned.mkl_start_next_year == 1].mkl_start_next_year.sum()\n",
    "print 'The number of observations where the country would have a \\\n",
    "mass killing start in the following year is: %s' % int(y_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Correlation heatmap for cleaned data\n",
    "dft = train_cleaned.corr()\n",
    "# Print correlation values against mkl_start\n",
    "print dft.mkl_start_next_year.drop(dft.index[0]).sort_values()\n",
    "# Observations:\n",
    "#    Negative corr (<-4.5%)\n",
    "#        pol_durable_ln        -0.060688\n",
    "#            Regime duration (makes sense, although surprised to see strongest neg corr here)\n",
    "#        pol_cat_fl3           -0.059782\n",
    "#            Democracy political category\n",
    "#        wdi_trade_ln          -0.048357\n",
    "#            Trade openness, logged\n",
    "#    Positive corr (>6%)\n",
    "#        pol_cat_fl7            0.060388\n",
    "#             Other political cat (not dem, autocracy, or anocracy) (suprise: anocracy )\n",
    "#        imr_normed_ln          0.061869\n",
    "#            Infant mortality rate, logged\n",
    "#        mev_civtot_ln          0.068543\n",
    "#            Scale of violent civil conflict, logged\n",
    "        \n",
    "sns.heatmap(dft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Details of modeling process, including how models were selected and validated\n",
    "===========================================================================================\n",
    "\n",
    "Logistic regression and RandomForests are used along with the pre-defined features. Confusion matrix helps understand and refine the classification results – want to capture more true positives to make sure my model can catch all mkl\\_starts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1. Define X and y\n",
    "feature_cols = ['reg_afr','reg_eap','reg_eur','reg_mna','reg_sca','reg_amr','mkl_ongoing',\\\n",
    "             'mkl_ever','countryage_ln','wdi_popsize_ln','imr_normed_ln','gdppcgrow_sr','wdi_trade_ln',\\\n",
    "             'ios_iccpr1','postcw','pol_cat_fl1','pol_cat_fl2','pol_cat_fl3','pol_cat_fl7','pol_durable_ln',\\\n",
    "             'dis_l4pop_ln','elf_ethnicc1','elf_ethnicc2','elf_ethnicc9','elc_eleth1','elc_eleth2','elc_eliti',\\\n",
    "             'cou_tries5d','pit_sftpuhvl2_10_ln','mev_regac_ln','mev_civtot_ln']\n",
    "X = train_cleaned[feature_cols]\n",
    "y = train_cleaned.mkl_start_next_year\n",
    "#y = train_cleaned.mkl_start\n",
    "\n",
    "print y.value_counts()\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2. Choose model and import\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] }\n",
    "# GridSearchCV can be used somehow to get the best C value to use.\n",
    "clf = GridSearchCV(LogisticRegression(penalty='l2'), param_grid)\n",
    "GridSearchCV(cv=None,\n",
    "       estimator=LogisticRegression(C=1.0, intercept_scaling=1, dual=False, fit_intercept=True,\n",
    "          penalty='l2', tol=0.0001),\n",
    "       param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]})\n",
    "\n",
    "# Find best C value using GridSearchCV\n",
    "clf.fit(X,y)\n",
    "clf.best_params_['C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 3. Instantiate\n",
    "# Increasing C>1 does not increase sensitivity but does increase FPs\n",
    "# Best C value from above is incorrect and leads to 0 TP\n",
    "#logreg = LogisticRegression(C=clf.best_params_['C'])\n",
    "# Manually found C value with max TP and min FP\n",
    "# Manually played with tol and could not improve sensitivity\n",
    "logreg = LogisticRegression(C=clf.best_params_['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 4. Fit model, 5. Predict, 6. Evaluate\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn import metrics, cross_validation\n",
    "# 10-fold cross-validation used to check model accuracy and keeps predicted results\n",
    "predicted = cross_validation.cross_val_predict(logreg, X, y, cv=10)\n",
    "print metrics.accuracy_score(y, predicted)\n",
    "pred = pd.DataFrame(predicted, columns=['Predicted_mkl_start'])\n",
    "pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The original dataset has 116 mkl_start. Prediction from model: 18\n",
    "# Note: When using y = mkl_start_next_year, predictions from model = 0. Not sure why.\n",
    "pred[pred.Predicted_mkl_start == 1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data_pred = pd.concat([info, y, pred, pred_proba.Predicted_proba_mkl_start, X], axis=1)\n",
    "data_pred = pd.concat([info, y, pred, X], axis=1)\n",
    "print data_pred.shape\n",
    "data_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show results using confusion matrix\n",
    "y_true = y\n",
    "y_pred = data_pred.Predicted_mkl_start\n",
    "df_confusion = pd.crosstab(y_true, y_pred)\n",
    "df_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sensitivity: When actual value is positive, how often is prediction correct?\n",
    "# AKA “True Positive Rate” or “Recall”\n",
    "# TP / (all actual positives)\n",
    "# If this value is close to 100%, model is near perfect\n",
    "logreg_sens = (df_confusion[1:2].unstack().sum()-df_confusion[0][1]*1.0)/df_confusion[1:2].unstack().sum()\n",
    "logreg_sens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a Graphviz file\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "\n",
    "def decision_tree_model(X,y,feature_cols,depth,verbose=False,dot_map=False):\n",
    "    '''Run decision tree model, print confusion matrix, returns senstivity'''\n",
    "    # If verbose == True, print everything\n",
    "    # 1. X and y defined\n",
    "    # 2. Import model\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    # 3. Instantiate a DecisionTreeRegressor (with random_state=1)\n",
    "    treereg = DecisionTreeRegressor(random_state=1)\n",
    "    # max_depth=3 was best, so fit a tree using that parameter\n",
    "    treereg = DecisionTreeRegressor(max_depth=depth, random_state=1)\n",
    "    \n",
    "    # 4. Fit model\n",
    "    treereg.fit(X, y)\n",
    "    \n",
    "    # \"Gini importance\" of each feature: the (normalized) total reduction of error brought by that feature\n",
    "    if verbose:\n",
    "        #print pd.DataFrame({'feature':feature_cols, 'importance':treereg.feature_importances_})\n",
    "        print pd.DataFrame({'feature':feature_cols, 'importance':treereg.feature_importances_}).sort_values('importance', ascending=False)\n",
    "    if dot_map:\n",
    "        export_graphviz(treereg, out_file='tree_vehicles_EWP_depth_'+str(depth)+'.dot', feature_names=feature_cols)\n",
    "    \n",
    "    # use fitted model to make predictions on testing data\n",
    "    # 5. Predict\n",
    "    X_test = X\n",
    "    y_test = y\n",
    "    y_pred = treereg.predict(X_test)\n",
    "    \n",
    "    print\n",
    "    \n",
    "    # 6. Evaluate\n",
    "    # calculate RMSE\n",
    "    print 'RMSE: %s' % np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    print\n",
    "    \n",
    "    # print confusion matrix\n",
    "    df_confusion = pd.crosstab(y_test, y_pred)\n",
    "    return df_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try decision trees with a range of depths\n",
    "for n in np.arange(2,25):\n",
    "    print 'depth: %s' % n\n",
    "    print decision_tree_model(X,y,feature_cols,n,verbose=False,dot_map=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# From above, a depth of 11 seams like a good predictor. It seems like this could be tuned to increase TPs,\n",
    "# without predicting too many FN.\n",
    "decision_tree_model(X,y,feature_cols,11,verbose=True,dot_map=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From looking at the resulting tree, the prediction will only ever be positive if the country in question has an ongoing state-led mass killing! This must be overfitting. At the very least, as time goes on and most countries have no ongoing MKL, this model would never predict any likelihood of that state having a new mass killing.\n",
    "\n",
    "![Tree Vehicles EWP depth 11 head](tree_vehicles_EWP_depth_11_head.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Tree Vehicles EWP depth 11 full](tree_vehicles_EWP_depth_11.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfclass = RandomForestClassifier()\n",
    "rfclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune n_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # list of values to try for n_estimators\n",
    "# #estimator_range = range(10, 50, 10)\n",
    "# #estimator_range = range(10, 310, 10)\n",
    "# estimator_range = range(10, 510, 10)\n",
    "\n",
    "# # list to store the average RMSE for each value of n_estimators\n",
    "# RMSE_scores = []\n",
    "\n",
    "# # use 5-fold cross-validation with each value of n_estimators (WARNING: SLOW!)\n",
    "# for estimator in estimator_range:\n",
    "#     rfclass = RandomForestClassifier(n_estimators=estimator, random_state=1)\n",
    "#     MSE_scores = cross_val_score(rfclass, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "#     # Retune after feature selection:\n",
    "#     #MSE_scores = cross_val_score(rfclass, X_important, y, cv=5, scoring='neg_mean_squared_error')\n",
    "#     RMSE_scores.append(np.mean(np.sqrt(-MSE_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # allow plots to appear in the notebook\n",
    "# %matplotlib inline\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # plot n_estimators (x-axis) versus RMSE (y-axis)\n",
    "# plt.plot(estimator_range, RMSE_scores)\n",
    "# plt.xlabel('n_estimators')\n",
    "# plt.ylabel('RMSE (lower is better)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![n_estimators for X](./images/rf_n_estimators for X.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # show the best RMSE and the corresponding max_features\n",
    "# best_nest = sorted(zip(RMSE_scores, estimator_range))[0][1]\n",
    "# print best_nest\n",
    "# sorted(zip(RMSE_scores, estimator_range))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The computations above take a long time and are therefore commented out with the result given below for speed.\n",
    "# Press \"CTRL + /\" to uncomment\n",
    "best_nest = 40\n",
    "# (0.10398748422060437, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning max_features\n",
    "Tune the number of *max_features* that should be tested at each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # list of values to try for max_features\n",
    "# feature_range = range(1, len(feature_cols)+1)\n",
    "\n",
    "# # list to store the average RMSE for each value of max_features\n",
    "# RMSE_scores = []\n",
    "\n",
    "# # use 10-fold cross-validation with each value of max_features (WARNING: SLOW!)\n",
    "# for feature in feature_range:\n",
    "#     rfclass = RandomForestClassifier(n_estimators=best_nest, max_features=feature, random_state=1)\n",
    "#     MSE_scores = cross_val_score(rfclass, X, y, cv=10, scoring='neg_mean_squared_error')\n",
    "#     RMSE_scores.append(np.mean(np.sqrt(-MSE_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # plot max_features (x-axis) versus RMSE (y-axis)\n",
    "# plt.plot(feature_range, RMSE_scores)\n",
    "# plt.xlabel('max_features')\n",
    "# plt.ylabel('RMSE (lower is better)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Max Features for X](./images/rf_max features for X.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # show the best RMSE and the corresponding max_features\n",
    "# best_maxf = sorted(zip(RMSE_scores, feature_range))[0][1]\n",
    "# print best_maxf\n",
    "# sorted(zip(RMSE_scores, feature_range))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The computations above take a long time and are therefore commented out with the result given below for speed.\n",
    "# Press \"CTRL + /\" to uncomment\n",
    "best_maxf = 1\n",
    "# (0.10550211145529216, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a Random Forest with the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# max_features=1 is best and n_estimators=40 is sufficiently large\n",
    "rfclass = RandomForestClassifier(n_estimators=best_nest, max_features=best_maxf, oob_score=True, random_state=1)\n",
    "rfclass.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute feature importances\n",
    "pd.DataFrame({'feature':feature_cols, 'importance':rfclass.feature_importances_}).sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfclass.feature_importances_.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute the out-of-bag R-squared score\n",
    "rfclass.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing X to its most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check the shape of X\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "sfm = SelectFromModel(rfclass,threshold=0.1, prefit=True)\n",
    "print(sfm.transform(X).shape[0],sfm.transform(X).shape[1])\n",
    "\n",
    "sfm = SelectFromModel(rfclass, threshold='mean', prefit=True)\n",
    "print(sfm.transform(X).shape[0],sfm.transform(X).shape[1])\n",
    "\n",
    "sfm = SelectFromModel(rfclass, threshold='median', prefit=True)\n",
    "print(sfm.transform(X).shape[0],sfm.transform(X).shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a new feature matrix that only includes important features\n",
    "#X_important = rfclass.transform(X, threshold='mean')\n",
    "#X_important.shape\n",
    "sfm = SelectFromModel(rfclass, threshold='mean', prefit=True)\n",
    "X_important = sfm.transform(X)\n",
    "print(X_important.shape[0],X_important.shape[1])\n",
    "sfm.get_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clacutate best max_features and n_estimators\n",
    "##### Tune n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # list of values to try for n_estimators\n",
    "# #estimator_range = range(10, 50, 10)\n",
    "# estimator_range = range(10, 510, 10)\n",
    "\n",
    "# # list to store the average RMSE for each value of n_estimators\n",
    "# RMSE_scores = []\n",
    "\n",
    "# # use 5-fold cross-validation with each value of n_estimators (WARNING: SLOW!)\n",
    "# for estimator in estimator_range:\n",
    "#     rfclass = RandomForestClassifier(n_estimators=estimator, random_state=1)\n",
    "#     # Retune using feature selection:\n",
    "#     MSE_scores = cross_val_score(rfclass, X_important, y, cv=5, scoring='neg_mean_squared_error')\n",
    "#     RMSE_scores.append(np.mean(np.sqrt(-MSE_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # allow plots to appear in the notebook\n",
    "# %matplotlib inline\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # plot n_estimators (x-axis) versus RMSE (y-axis)\n",
    "# plt.plot(estimator_range, RMSE_scores)\n",
    "# plt.xlabel('n_estimators')\n",
    "# plt.ylabel('RMSE (lower is better)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![n_estimators for X important](./images/rf_n_estimators for X important.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # show the best RMSE and the corresponding max_features\n",
    "# best_nest_imp = sorted(zip(RMSE_scores, estimator_range))[0][1]\n",
    "# print best_nest_imp\n",
    "# sorted(zip(RMSE_scores, estimator_range))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The computations above take a long time and are therefore commented out with the result given below for speed.\n",
    "# Press \"CTRL + /\" to uncomment\n",
    "best_nest_imp = 360\n",
    "# (0.10704178621722052, 360)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tune max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # list of values to try for max_features\n",
    "# feature_range = range(1, len(X_important[0])+1)\n",
    "\n",
    "# # list to store the average RMSE for each value of max_features\n",
    "# RMSE_scores = []\n",
    "\n",
    "# # use 10-fold cross-validation with each value of max_features (WARNING: SLOW!)\n",
    "# for feature in feature_range:\n",
    "#     rfclass = RandomForestClassifier(n_estimators=best_nest_imp, max_features=feature, random_state=1)\n",
    "#     MSE_scores = cross_val_score(rfclass, X_important, y, cv=10, scoring='neg_mean_squared_error')\n",
    "#     RMSE_scores.append(np.mean(np.sqrt(-MSE_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # plot max_features (x-axis) versus RMSE (y-axis)\n",
    "# plt.plot(feature_range, RMSE_scores)\n",
    "# plt.xlabel('max_features')\n",
    "# plt.ylabel('RMSE (lower is better)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![max_features for X](./images/rf_max features for X important.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # show the best RMSE and the corresponding max_features\n",
    "# best_maxf_imp = sorted(zip(RMSE_scores, feature_range))[0][1]\n",
    "# print best_maxf\n",
    "# sorted(zip(RMSE_scores, feature_range))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The computations above take a long time and are therefore commented out with the result given below for speed.\n",
    "# Press \"CTRL + /\" to uncomment\n",
    "best_maxf_imp = 1\n",
    "# (0.10396699975444945, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Random Forest models using only important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check the RMSE for a Random Forest that only includes important features\n",
    "rfclass = RandomForestClassifier(n_estimators=best_nest, max_features=best_maxf, random_state=1)\n",
    "scores = cross_val_score(rfclass, X_important, y, cv=10, scoring='neg_mean_squared_error')\n",
    "print np.mean(np.sqrt(-scores))\n",
    "rfclass.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "# 10-fold cross-validation used to check model accuracy and keeps predicted results\n",
    "rf_proba = cross_val_predict(rfclass, X_important, y, cv=10, method='predict_proba')\n",
    "#proba = cross_val_predict(logreg, X, y, cv=cv, method='predict_proba')\n",
    "\n",
    "pred = pd.DataFrame(rf_proba, columns=[['mkl_next_0','mkl_next_1']])\n",
    "pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute feature importances\n",
    "#pd.DataFrame({'feature':feature_cols, 'importance':rf_proba.feature_importances_}).sort_values('importance', ascending=False)\n",
    "print pred.shape\n",
    "\n",
    "# Fix index to account for rows that were dropped\n",
    "pred.index = train_cleaned.index\n",
    "pred[188:198]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#rfclass.fit(X_important,y)\n",
    "y_pred_proba = pred[pred.mkl_next_1 > 0.5]\n",
    "#pd.DataFrame(rfclass.predict_proba(X))\n",
    "\n",
    "print y_pred_proba.shape\n",
    "y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The original dataset has 116 mkl_start. Prediction from model: 10 with >50% probability\n",
    "pred[pred.mkl_next_1 > 0.5].mkl_next_1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregated Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = pd.concat([train_cleaned, pred.mkl_next_1], axis=1)\n",
    "results.rename(columns={\"mkl_next_1\": \"rf_proba_mkl_start_next_year\"}, inplace = True)\n",
    "dft = results[results.mkl_start_next_year == 1][['country','mkl_start_next_year','rf_proba_mkl_start_next_year']]\n",
    "print dft.rf_proba_mkl_start_next_year.describe()\n",
    "print results.rf_proba_mkl_start_next_year.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show a histogram of the Random Forests predicted probability for countries that would have a mkl_start next year\n",
    "results[results.mkl_start_next_year == 1].rf_proba_mkl_start_next_year.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results.rf_proba_mkl_start_next_year.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp1 = results[results.rf_proba_mkl_start_next_year > 0.3][['country','year','mkl_start_next_year','rf_proba_mkl_start_next_year']]\n",
    "print temp1.mkl_start_next_year.value_counts()\n",
    "temp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X_important.shape\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Best n_est: 40\n",
    "# Best max_f: 1\n",
    "def rf_class_predictions(train_cleaned, feature_matrix, y, n_est, max_f):\n",
    "    ''' The funciton will create a dataframe with predicted \\\n",
    "    probability for the response given the random forest parameters'''\n",
    "    rfclass = RandomForestClassifier(n_estimators=n_est, max_features=max_f, random_state=1)\n",
    "\n",
    "    # 10-fold cross-validation used to check model accuracy and keeps predicted results\n",
    "    rf_proba = cross_val_predict(rfclass, feature_matrix, y, cv=10, method='predict_proba')\n",
    "    #proba = cross_val_predict(logreg, X, y, cv=cv, method='predict_proba')\n",
    "\n",
    "    pred = pd.DataFrame(rf_proba, columns=[['rf_proba_0','rf_proba']])\n",
    "    pred.index = train_cleaned.index\n",
    "    #pred.rename(columns={\"mkl_next_1\": \"rf_proba\"}, inplace = True)\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_results(data_in,prediction,pred_name):\n",
    "    ''' This function combines the prediction with the larger dataset. '''\n",
    "    # Change the final column of the prediction dataframe to the name given\n",
    "    prediction.rename(columns={prediction.columns[-1]: pred_name}, inplace = True)\n",
    "    # Keep only the last column of the predicted df, that is predicted 1\n",
    "    combo = pd.concat([data_in, prediction.iloc[:,-1]], axis=1)\n",
    "    # Remove any duplicate columns\n",
    "    combo = combo.T.drop_duplicates().T\n",
    "    return combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from sklearn.metrics import confusion_matrix\n",
    "def confusion_matrix_proba(y,y_pred,**keyword_parameters):\n",
    "    ''' Create confusion matrix:\n",
    "    y_true: measured binary result (Pandas Series)\n",
    "    y_pred: predicted binary result or predicted probability if threshold exists (Pandas Series)\n",
    "    theshold: Optional tuning parameter for making prediction based on pediction probability'''\n",
    "    # Create a series based on the threshold value if given\n",
    "    if('threshold' in keyword_parameters):\n",
    "        y_pred = y_pred.map(lambda x: 1 if (x > float(keyword_parameters['threshold'])) else 0)\n",
    "    # Create confusion matrix\n",
    "    df_confusion = pd.crosstab(y, y_pred)\n",
    "    # Great resource: https://classeval.wordpress.com/introduction/basic-evaluation-measures/\n",
    "    # If the shape is 2x2, there are predictions for 1 and 0, calc TP and FP\n",
    "    if (df_confusion.shape == (2,2)):\n",
    "        df_confusion.TP = TP = df_confusion[1][1]\n",
    "        df_confusion.FP = FP = df_confusion[1][0]\n",
    "        df_confusion.TN = TN = df_confusion[0][0]\n",
    "        df_confusion.FN = FN = df_confusion[0][1]\n",
    "    # If 2x1 and column is pred_0 then no predictions for positive, TP = FP = 0\n",
    "    if((df_confusion.shape == (2, 1)) & (df_confusion.columns[0] == 0)):\n",
    "        df_confusion.TP = TP = 0\n",
    "        df_confusion.FP = FP = 0\n",
    "        df_confusion.TN = TN = df_confusion[0][0]\n",
    "        df_confusion.FN = FN = df_confusion[0][1]\n",
    "    # If 2x1 and column is pred_1 then no predictions for negative, TN = FN = 0\n",
    "    if((df_confusion.shape == (2, 1)) & (df_confusion.columns[0] == 1)):\n",
    "        df_confusion.TP = TP = df_confusion.iloc[1,0]\n",
    "        df_confusion.FP = FP = df_confusion.iloc[0,0]\n",
    "        df_confusion.TN = TN = 0\n",
    "        df_confusion.FN = FN = 0\n",
    "\n",
    "    df_confusion.sensitivity = 1.0 * TP/(TP + FN) # result of TP/P\n",
    "    df_confusion.ERR = 1.0 * (FP + FN)/((TP + FN) + (TN + FP)) #(FP + FN)/(P + N)\n",
    "    df_confusion.accuracy = 1.0 * (TP + TN)/((TP + FN) + (TN + FP)) #(TP + TN)/(P + N)\n",
    "    return df_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true,y_pred_proba):\n",
    "    ''' This function will compute the sensitivity and error rates for a model\\\n",
    "    over a threshold from 0.00 to 0.99, with 0.01 steps'''\n",
    "    size = range(0,101,1)\n",
    "    df_sens = pd.DataFrame(columns=['threshold','sensitivity','ERR','accuracy'],index=np.arange(0.00,1.00,0.01))\n",
    "    for num in size:\n",
    "        dft = confusion_matrix_proba(y_true,y_pred_proba, threshold=(num/100.0))\n",
    "        df_sens.iloc[num-1]=[num/100.0,dft.sensitivity,dft.ERR,dft.accuracy]\n",
    "    return df_sens[['sensitivity','ERR','accuracy']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using the important features for refining the model, now using n_est = 270, not 40\n",
    "rf_pred = rf_class_predictions(train_cleaned, X_important, y, 270, best_maxf)\n",
    "dft = combine_results(train_cleaned[['country','year','mkl_start_next_year']],rf_pred,'rf_pred_1')\n",
    "dft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_pred = rf_class_predictions(train_cleaned, X_important, y, 10, best_maxf)\n",
    "dft = combine_results(dft,rf_pred,'rf_pred_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_pred = rf_class_predictions(train_cleaned, X_important, y, 40, best_maxf)\n",
    "dft = combine_results(dft,rf_pred,'rf_pred_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_pred = rf_class_predictions(train_cleaned, X_important, y, 150, best_maxf)\n",
    "dft = combine_results(dft,rf_pred,'rf_pred_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_pred = rf_class_predictions(train_cleaned, X_important, y, 300, best_maxf)\n",
    "dft = combine_results(dft,rf_pred,'rf_pred_5')\n",
    "dft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dft[dft.rf_pred_1 > 0].mkl_start_next_year.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dft[dft.rf_pred_2 > 0].mkl_start_next_year.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dft[dft.rf_pred_3 > 0].mkl_start_next_year.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dft[dft.rf_pred_4 > 0].mkl_start_next_year.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dft[dft.rf_pred_5 > 0].mkl_start_next_year.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Using the important features for refining the model, now using n_est = 270, not 40\n",
    "# rf_pred = rf_class_predictions(train_cleaned, X_important, y, 270, best_maxf)\n",
    "# results = combine_results(train_cleaned[['country','year','mkl_start_next_year']],rf_pred,'rf_pred_proba')\n",
    "results = dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results.rf_pred_1.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show a histogram of the Random Forests predicted probability > 0 for countries that would have a mkl_start next year\n",
    "temp = results[(results.mkl_start_next_year == 1) & (results.rf_pred_1 > 0)].rf_pred_1\n",
    "temp.hist()\n",
    "temp.shape\n",
    "# Results have captured 85/98 True Positives!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp = results[(results.mkl_start_next_year == 0) & (results.rf_pred_1 > 0)].rf_pred_1\n",
    "temp.hist()\n",
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dft = confusion_matrix_proba(y,results.rf_pred_1, threshold='0.0')\n",
    "print \"Sensitivity: %s\" % dft.sensitivity\n",
    "print \"Error Rate: %s\" % dft.ERR\n",
    "dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dft = confusion_matrix_proba(y,results.rf_pred_1, threshold='0.01')\n",
    "print \"Sensitivity: %s\" % dft.sensitivity\n",
    "print \"Error Rate: %s\" % dft.ERR\n",
    "dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dft = confusion_matrix_proba(y,results.rf_pred_1, threshold='0.1')\n",
    "print \"Sensitivity: %s\" % dft.sensitivity\n",
    "print \"Error Rate: %s\" % dft.ERR\n",
    "dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dft = confusion_matrix_proba(y,results.rf_pred_1, threshold='0.5')\n",
    "print \"Sensitivity: %s\" % dft.sensitivity\n",
    "print \"Error Rate: %s\" % dft.ERR\n",
    "dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dft = confusion_matrix_proba(y,results.rf_pred_1, threshold='0.7')\n",
    "print \"Sensitivity: %s\" % dft.sensitivity\n",
    "print \"Error Rate: %s\" % dft.ERR\n",
    "dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dft = confusion_matrix_proba(y,results.rf_pred_1, threshold='0.9')\n",
    "print \"Sensitivity: %s\" % dft.sensitivity\n",
    "print \"Error Rate: %s\" % dft.ERR\n",
    "dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dft = plot_confusion_matrix(y,results.rf_pred_1)\n",
    "dft2 = dft[(dft.ERR<0.5) & (dft.sensitivity == dft[dft.ERR<0.5].sensitivity.max())]\n",
    "print dft2\n",
    "print (dft2.sensitivity * dft2.accuracy)[0]\n",
    "dft.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dft = plot_confusion_matrix(y,results.rf_pred_2)\n",
    "dft2 = dft[(dft.ERR<0.5) & (dft.sensitivity == dft[dft.ERR<0.5].sensitivity.max())]\n",
    "print dft2\n",
    "print (dft2.sensitivity * dft2.accuracy)[0]\n",
    "dft.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dft = plot_confusion_matrix(y,results.rf_pred_3)\n",
    "dft2 = dft[(dft.ERR<0.5) & (dft.sensitivity == dft[dft.ERR<0.5].sensitivity.max())]\n",
    "print dft2\n",
    "print (dft2.sensitivity * dft2.accuracy)[0]\n",
    "dft.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dft = plot_confusion_matrix(y,results.rf_pred_4)\n",
    "dft2 = dft[(dft.ERR<0.5) & (dft.sensitivity == dft[dft.ERR<0.5].sensitivity.max())]\n",
    "print dft2\n",
    "print (dft2.sensitivity * dft2.accuracy)[0]\n",
    "dft.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dft = plot_confusion_matrix(y,results.rf_pred_5)\n",
    "dft2 = dft[(dft.ERR<0.5) & (dft.sensitivity == dft[dft.ERR<0.5].sensitivity.max())]\n",
    "print dft2\n",
    "print (dft2.sensitivity * dft2.accuracy)[0]\n",
    "dft.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#del results['rf_pred_features_3']\n",
    "# del results['rf_pred_7_features']\n",
    "#del results['logreg_pred_1']\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add logreg to result df, add KNN?\n",
    "# Avg results?\n",
    "\n",
    "#countryage_ln\t0.119162\n",
    "#9\twdi_popsize_ln\t0.106623\n",
    "#10\timr_normed_ln\t0.100250\n",
    "#29\tmev_regac_ln\t0.084472\n",
    "#19\tpol_durable_ln\t0.083646\n",
    "#11\tgdppcgrow_sr\t0.076864\n",
    "#12\twdi_trade_ln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_3 = ['countryage_ln','wdi_popsize_ln','imr_normed_ln']\n",
    "rf_pred = rf_class_predictions(train_cleaned, train_cleaned[features_3], y, 270, best_maxf)\n",
    "results = combine_results(results,rf_pred,'rf_pred_features_3')\n",
    "dft = plot_confusion_matrix(y,results.rf_pred_features_3)\n",
    "dft2 = dft[(dft.ERR<0.5) & (dft.sensitivity == dft[dft.ERR<0.5].sensitivity.max())]\n",
    "print dft2\n",
    "print (dft2.sensitivity * dft2.accuracy)[0]\n",
    "dft.plot()\n",
    "\n",
    "# rf_pred = rf_class_predictions(train_cleaned, X_important, y, 40, best_maxf)\n",
    "# dft = combine_results(dft,rf_pred,'rf_pred_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_7 = ['countryage_ln','wdi_popsize_ln','imr_normed_ln','mev_regac_ln','pol_durable_ln','gdppcgrow_sr','wdi_trade_ln']\n",
    "rf_pred = rf_class_predictions(train_cleaned, train_cleaned[features_7], y, 270, best_maxf)\n",
    "results = combine_results(results,rf_pred,'rf_pred_features_7')\n",
    "dft = plot_confusion_matrix(y,results.rf_pred_features_7)\n",
    "dft2 = dft[(dft.ERR<0.5) & (dft.sensitivity == dft[dft.ERR<0.5].sensitivity.max())]\n",
    "print dft2\n",
    "print (dft2.sensitivity * dft2.accuracy)[0]\n",
    "dft.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression, round 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logreg_class_predictions(train_cleaned, feature_matrix, y):\n",
    "    ''' The funciton will create a dataframe with predicted \\\n",
    "     probability for the response given the logistic regression parameters'''\n",
    "    param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] }\n",
    "    # GridSearchCV can be used to get the best C value to use.\n",
    "    clf = GridSearchCV(LogisticRegression(penalty='l2'), param_grid)\n",
    "    GridSearchCV(cv=None,\n",
    "           estimator=LogisticRegression(C=1.0, intercept_scaling=1, dual=False, fit_intercept=True,\n",
    "              penalty='l2', tol=0.0001),\n",
    "           param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]})\n",
    "\n",
    "    # Find best C value using GridSearchCV\n",
    "    clf.fit(feature_matrix,y)\n",
    "\n",
    "    # 3. Instantiate\n",
    "    # Increasing C>1 does not increase sensitivity but does increase FPs\n",
    "    # Best C value from above is incorrect and leads to 0 TP\n",
    "    #logreg = LogisticRegression(C=clf.best_params_['C'])\n",
    "    # Manually found C value with max TP and min FP\n",
    "    # Manually played with tol and could not improve sensitivity\n",
    "    logreg = LogisticRegression(C=clf.best_params_['C'])\n",
    "\n",
    "    # 4. Fit model, 5. Predict, 6. Evaluate\n",
    "    # 10-fold cross-validation used to check model accuracy and keeps predicted results\n",
    "    logreg_proba = cross_val_predict(logreg, feature_matrix, y, cv=10, method='predict_proba')\n",
    "\n",
    "    pred = pd.DataFrame(logreg_proba, columns=[['logreg_proba_0','logreg_proba']])\n",
    "    pred.index = train_cleaned.index\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make logreg predictions using X_important (from RF)\n",
    "logreg_pred = logreg_class_predictions(train_cleaned, X_important, y)\n",
    "results = combine_results(results,logreg_pred,'logreg_proba_1')\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dft = plot_confusion_matrix(y,results.logreg_proba_1)\n",
    "\n",
    "dft2 = dft[(dft.ERR<0.5) & (dft.sensitivity == dft[dft.ERR<0.5].sensitivity.max())]\n",
    "print dft2\n",
    "print (dft2.sensitivity * dft2.accuracy)\n",
    "dft.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logreg_pred = logreg_class_predictions(train_cleaned, train_cleaned[features_3], y)\n",
    "results = combine_results(results,logreg_pred,'logreg_proba_features_3')\n",
    "\n",
    "dft = plot_confusion_matrix(y,results.logreg_proba_features_3)\n",
    "\n",
    "dft2 = dft[(dft.ERR<0.5) & (dft.sensitivity == dft[dft.ERR<0.5].sensitivity.max())]\n",
    "print dft2\n",
    "print (dft2.sensitivity * dft2.accuracy)\n",
    "dft.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logreg_pred = logreg_class_predictions(train_cleaned, train_cleaned[features_7], y)\n",
    "results = combine_results(results,logreg_pred,'logreg_proba_features_7')\n",
    "\n",
    "dft = plot_confusion_matrix(y,results.logreg_proba_features_7)\n",
    "\n",
    "dft2 = dft[(dft.ERR<0.5) & (dft.sensitivity == dft[dft.ERR<0.5].sensitivity.max())]\n",
    "print dft2\n",
    "print (dft2.sensitivity * dft2.accuracy)\n",
    "dft.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"From the results above, LogisticRegression performed significantly better \\\n",
    "than RandomForestsClassifier in that logreg was able to predict 2 true positives \\\n",
    "(sensitivity of %s%%) and rfclass predicted 0 TP.\" % round(logreg_sens*100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"The logreg prediction is %s%% higher than the null accuracy of %s%%\" \\\n",
    "% (round((logreg_sens-null_accuracy)*100,2),round(null_accuracy*100,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Challenges and successes\n",
    "=============================\n",
    "\n",
    "Making predictions for a response that is very uncommon (116/9330 observations are positive) makes it difficult to test this model.\n",
    "\n",
    "Nulls may require categorical modelling (KNN, DBSCAN clustering, ideally, model values for each country.) If possible, use info about which countries became other countries (or use correlations between yrborn and yrdied to assume association when possible). Also, model year over year change per feature for making estimates for features (instead of imputing values using mean)?\n",
    "\n",
    "In addition to the models implemented here, I also attempted PCA to minimize features but hit a road block in computing a confusion matrix (and also did not trust the results)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Possible extensions or business applications of your project\n",
    "============================================================\n",
    "\n",
    "Help further refine Early Warning Project models to influence results.\n",
    "\n",
    "Treat as time series and predict by year?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Conclusions and key learnings\n",
    "=============================\n",
    "\n",
    "Certain features had high corr with new mkl:\n",
    "\n",
    "-   Blah\n",
    "\n",
    "-   Blah\n",
    "\n",
    "Certain additional features were added to model to increase accuracy:\n",
    "\n",
    "-   Blahg\n",
    "\n",
    "Certain features were unimportant and therefore removed:\n",
    "\n",
    "-   That one\n",
    "\n",
    "Model not very predictive, just indicative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "==========\n",
    "\n",
    "<sup>1</sup>\n",
    "<http://survivors-fund.org.uk/resources/rwandan-history/statistics/>  \n",
    "<sup>2</sup>Preventing Genocide, A Blueprint for U.S. Policymakers,\n",
    "Madeleine K. Albright, William S. Cohen  \n",
    "<https://www.usip.org/genocide-prevention-task-force/view-report>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
